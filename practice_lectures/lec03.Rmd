---
title: "Practice Lecture 3 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "February 14, 2019"
---


## Matrix operations in R

R can do all the standard matrix operations. Let's go through them quickly. First initialize two example matrices:

```{r}
A = matrix(rep(1, 4), nrow = 2)
A
B = array(seq(1, 4), dim = c(2, 2))
B
I = diag(2) #create an identity matrix of size 2x2
I
```

Now we show off some operations:

```{r}
A * B #element-wise multiplication
A %*% B #matrix multiplication
B %*% I
t(B) #transpose
solve(B)
solve(A) #BOOM - why?
solve(I)
#rank(A) = 1 #no such function... but... there are tons of add-on libraries for matrix computations e.g.
pacman::p_load(Matrix) #load the Matrix library
rankMatrix(B)
rankMatrix(A)
rankMatrix(I)
```

Note that vectors and matrices are not the same:

```{r}
v = c(1, 2, 3) #3-d vector
t(v) #converts to 1x3 vector... unsure why
t(t(v))
v %*% v #seems to default to dot product
t(v) %*% t(t(v)) #dot product
I = diag(3)
I %*% v #seems to default correctly!
I %*% t(v) #actually uncomformable
```

## The linear threshold model

Using the algorithm $\mathcal{A}$ discussed previously, this model was dubbed the "perceptron". However, there are other algorithms $\mathcal{A}$ we can use to fit the "best" model in $\mathcal{H}$.

We spoke about how the `w` vector is in a large space, $R^p$. A course in optimization will describe methods how to find optimal and approximately optimal solutions for `w` based on an "objective function" or "fitness function" or "cost function". Here that target is the number of total errors of the `n` examples in the training data set.

```{r}
SAE = function(w_vec){
  yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
  sum(y_binary != yhat)
}
```


Here are some off-the-shelf solvers in R in action. Note: this is not the classic "perceptron learning algorithm". Also: this is complete junk made-up data so it's only an illustration. You will see real data for your homework.

The first is `nlm` which uses some sort of Newton-like algorithm.

```{r}
?nlm
w_vec = nlm(SAE, c(1, 1))$estimate #doesn't work at all ... I think because our cost function is not continuous
w_vec
```
That didn't seem to work at all!

What is our error rate?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Garbage... because it didn't work. Anyone know why?

Let's try another optimization algorithm called Nelder-Mead developed by John Nelder and Roger Mead (both were famous statisticians) in 1965.

```{r}
pacman::p_load(optimx)
?optimx
optim_output = optimx(c(50, -10), SAE, method = "Nelder-Mead")
optim_output
w_vec = t(as.matrix(optim_output[1:2]))

```

What is our error rate using the Nelder-Mead local optimum?

```{r}
yhat = ifelse(X1 %*% w_vec > 0, 1, 0)
sum(y_binary != yhat) / length(y_binary)
```

Not good. Let's try again with the perceptron input as a starting point.

Lesson: perceptron learning algorithm maybe not so great. Also - different $\mathcal{A}$'s give you wildly different models $g$ even with the same allowable functions provided in $\mathcal{H}$. 

Most of the really creative work in modeling is within $\mathcal{A}$ although specifying $\mathcal{H}$ suitably flexible is very important too.

## Nearest Neighbor algorithm

Load up the breast cancer data set again.

```{r}
Xy = na.omit(MASS::biopsy) #The "breast cancer" data with all observations with missing values dropped
X = Xy[, 2 : 10] #V1, V2, ..., V9
y_binary = as.numeric(Xy$class == "malignant")
```

In one dimension, we are looking for the closest x. Here is a simple function that does it:

```{r}
nn_function = function(x_star){
  best_sqd_distance = Inf #good place to begin
  i_star = NA
  for (i in 1 : nrow(X)){
    dsqd = (X[i, 1] - x_star)^2
    if (dsqd < best_sqd_distance){
      best_sqd_distance = dsqd
      i_star = i
    }
  }
  y_binary[i_star]
}
nn_function(7.8)
nn_function(5.2)
```

Why is this silly for this dataset?

```{r}
str(X)
```

The features are not truly continuous. Would it make sense in higher dimensions? Your homework...

Has this been coded before? Definitely...

```{r}
pacman::p_load(class)
?knn
```

We can fit a knn model *and* predict in one shot via:

```{r}
y_hat = knn(X, c(4, 2, 1, 1, 2, 1, 2, 1, 1), y_binary, k = 1)
y_hat
```

Why is build model and predict in one shot natural in knn?

Now for an interesting exercise that will setup future classes:

```{r}
y_hat = knn(X, X, y_binary, k = 1)
y_hat
all.equal(y_hat, factor(y_binary))
```

No errors! Can this be a good model? No... "something" must be wrong! It is too good to be true.

Something is wrong. This is the first example of "overfitting". We will explore this in depth.

```{r}
rm(list = ls())
```


## Support Vector Machines (SVM)

You will code a basic SVM for homework. Here we use the `e1071` library.

```{r}
pacman::p_load(e1071)
```

We make a simple dataset first.

```{r}
Xy_simple = data.frame(
 response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```

We haven't spoken about visualization yet, but it is important we do some of it now. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)
```

And let's plot the data:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Now we fit a linear SVM. Since it is linearly separable, we can make $\lambda = 0$. Since the package doesn't allow zero, we make it trivially small. 

```{r}
Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
lambda = 1e-9
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = (2 * n * lambda)^-1, scale = FALSE)
```

Now we calculate the weight vector. This is technical and not covered in the class yet (or maybe never):

```{r}
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)
w_vec_simple_svm
```

and plot it:

```{r}
simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_svm_line
```

To understand the hyperparameter, let's introduce another data point so the training data is no longer linearly separable.

```{r}
Xy_simple = rbind(Xy_simple, c(0, 3.2, 3.2))
```

and plot it:

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

Let's try SVM at different $\lambda$ values.

```{r}
Xy_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
lambda = 1.3
n = nrow(Xy_simple_feature_matrix)
svm_model = svm(Xy_simple_feature_matrix, Xy_simple$response, kernel = "linear", cost = (2 * n * lambda)^-1, scale = FALSE)
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% Xy_simple_feature_matrix[svm_model$index, ] # the other terms
)

simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_svm_line
```

What lesson did we learn here? This hyperparameter really matters! We need to figure out a way to deal with this later. So far neither the perceptron nor the SVM is an algorithm without flaws.


## Errors and Warnings

You can write better functions if you make use of errors and warnings. Java forces you to catch likely errors via the "throws" designation for a method but there is no such requirement in R.

* Errors are unrecoverable, they halt execution i.e. red lights
* Warnings (under usual execution) do not halt execution, but they display a message, i.e. yellow lights

Here's how they work:

```{r}
my_vector_sum = function(xs){
  
  if (!(class(xs) %in% c("numeric", "integer"))){ #short for class(xs) == "numeric" | class(xs) == "integer"
    stop("You need to pass in a vector of numbers not a vector of type \"", class(xs), "\".\n") #throw error!
    #warning("Your vector of type \"", class(xs), "\" will be coerced to numbers.\n") #throw warning!
    xs = as.numeric(as.factor(xs))
  }
  
  tot = 0
  for (x in xs){
    tot = tot + x
  }
  tot
}
my_vector_sum(c(1, 2, 3))
my_vector_sum(c("a", "b", "c"))
```

There is a try-catch as well:

```{r}
xs = c("a", "b", "c")
tot = my_vector_sum(xs)

tot = tryCatch(
  {
    my_vector_sum(xs)
  },
  error = function(e){
    print("xs was non-numeric... coercing xs to numeric...")
    my_vector_sum(as.numeric(as.factor(xs)))
  }
)
tot
```
